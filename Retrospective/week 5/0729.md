# 07.29 (Week 5, Day 1)
## Review
### Done
- W4M2 환경셋팅 끝
- 오전 수업

    #### RDD
    - RDD를 추상화 한 것이 dataframe
    - RDD는 병렬처리가 가능한, 복원 가능한 것
    - 한번 만들면 수정할 수 없다. (immutable, hdfs에서와 같은 이유. consistency를 보장하기 어렵기 때문)

    #### Creating RDD

    1. External datasets
    2. Existing RDDs

    #### Transformation vs Action

    - Action을 수행하면 dataset에 대한 연산 결과를 return다.
    - Transformation은?
        - 연산만 저장한다. (graph 형태(DAG)로)
        - 이걸 RDD Lineage 라고 부른다.
    - Transformation 연산을 계속 저장하다가, action을 부를 때 연산을 수행한다. ➡️ Lazy Evaluation
    - 그 시간에, execution plan을 optimize한다.

    #### Narrow & Wide Transformations
    - Narrow는 하나의 partition, wide는 multiple partition.
    - Narrow는 shuffling 하지 않고, wide는 shuffling 한다.
    - shuffling이 spark에서 가장 비싼 연산이다.

    #### Partition
    - 데이터를 나눈 것

    #### Shuffle in Spark
    - 데이터를 분배하는 것

    #### How Spark Works Internally
    - 우리가 짜는게 driver program

    #### Executing Spark Jobs
    - action을 부르는 순간 job이 생성된다.
    - DAG scheduler가 DAG를 task로 쪼갠다.
    - 이후엔 task scheduler가 관리
    - spark context가 모든 node에 같이 간다.
    - Stage를 어떻게 구성할 지는 사용자(의 코드)가 결정한다. 여기서 성능차이가 발생 가능. Stage를 적게 만들수록 빠르다.

    #### Job
    - 잡 하나가 DAG 하나와 대응된다.

    #### Stage and Task
    - job은 여러개의 stage로 쪼개지며, stage 간에는 dependency가 존재한다.
    - 각 task는 single partition of data에 mapping 된다.
    - data를 partition으로 나누는 것도, partition에 mapping할 task를 만드는 것도 모두 spark가 알아서 결정한다. 사용자가 관여할 수 있는 건 stage 구성 뿐.

    #### Tasks and Stages in Spark
    - stage 사이에는 shuffling이 존재한다. 그래서 stage가 많아지면 느려지는 것
    - stage가 시작하기 전에, data는 이미 shuffling이 끝나 있어야 함
    - 최적화를 위해서는 shuffling을 줄여야 함

    #### Shared Variables in Spark
    - 두 가지 타입의 shared variable 존재
        - broadcast variable
        - accumulator
        
### Team Review
- 리뷰
    - W4M2 환경 셋팅 완료.
    - 사이드 프로젝트 정리 (WAV 차량 수요 분석 / 시각화 )
- Keep Problem Try
    - Keep
        - 시각화 하기 전에 의견을 충분히 공유해서, 같은 format으로 시각화 할 수 있었다.
    - Problem
        - 시각화했는데 맞게 했는지 / 의미있는 시각화 결과인지 판단하기 어렵다.
        - 분명히 처음에는 수요 예측하는 모델까지 간단하게 만들어보려 했으나, 정작 해본건 유의미한 상관관계가 있는 feature 선정하기에서 끝난 것이 아쉽다.
    - Try
        - Mission2 관련해서 좀 더 세분화하여 진행해보며 추가적인 시각화를 진행해보고자 한다.

### Side Project Review
#### Intro

- 미국에서는 매년 7월을 ‘장애인을 위한 달’ / ‘장애인 인식 개선을 위한 달’로써 행사를 많이 진행한다.
- 해당 사이드 프로젝트의 최종 목표인 ‘WAV 차량 수요 분석 및 예측’을 위해서는 ML/DL 기반의 예측 모델이 필요하다. 학습에 필요한 컬럼을 선별하는 과정에서 어떤 파생변수를 추가할 수 있을지에 대하여 목표를 두고 시각화를 진행하였음.

#### 평일 / 주말별 수요 예측 시각화 (groupBy Day)
<img width="1318" alt="image" src="https://github.com/user-attachments/assets/9490bc6d-e533-43db-8a42-b70fabd54248">

- 평일은 주황색, 주말은 하늘색으로 막대그래프의 색상을 변경하여 시각화에 활용하였음.
- 평일 / 주말 별로 WAV 차량 수요 예측 결과를 시각화하기 위해 연속적인 3달 (6, 7, 8)을 추출하여 경향성을 파악해보았으며, 모든 달에서 평일이 주말보다 더 많은 사용자가 있다는 사실을 인지함
- 이에 따라, 특정 일시 (ex - 20240701)에 수요 예측을 진행하고 싶다면 평일 / 주말 여부가 중요한 역할을 한다고 판단했으며, 해당 파생변수를 활용할 수 있을 것이라 예상함.

#### 계절(여름/겨울)별  수요 예측 시각화 (groupBy Hour)
<img width="1146" alt="image" src="https://github.com/user-attachments/assets/93b3b975-1401-4937-a696-6d85bfea6650">

- 위 - 여름 (2023년 7월) / 아래 - 겨울 (2023년 1월)
- 좌 - 평일 / 우 - 주말
- 여름과 겨울 모두 평일보다 주말에, 늦은 시간대의 이용 횟수가 높다.
- 여름, 겨울 모두 주말/평일에 시간대별 수요가 눈에 띄게 달라지므로, 시간대별 수요 예측을 진행한다면 평일/주말 여부도 예측을 위한 feature로 사용해야 할 것이다.

#### 날씨 관련 
- (TO DO)

### KPT & ETC
- 드디어 dockerfile을 제대로 설정했다. 
    - 하나의 컨테이너 안에서 hadoop과 spark를 모두 사용해야 하니 이 둘의 역할을 명확히 구분하고 싶었다.
    - spark_user와 hduser를 만들었고, 이 둘의 권한을 확실히 구분했다.
    - 하지만, 여전히 몇 가지 걸리는 점이 있다.
        - Dockerfile에서 user 별로 환경 변수를 설정하고 싶은데, 생각처럼 되지 않았다. 오늘 확인해보니, user 별 '.bashrc' 파일을 이용해서 할 수 있더라.
        - entrypoint에서, spark_user로 spark를 실행하고 난 후, user를 hduser로 변경해서 hdfs를 켜도록 했다. 이때, password가 필요해서 shell script에 hduser의 password를 기록하는 방식으로 이용했다. 이런 방식으로 사용해도 되나?
        - home dir 안의 user directory의 ownership을 각 user에게 주는 것이 일반적인지, 아니면 모든 권한을 주지만 ownership은 root가 가지고 있는게 맞는 건지 헷갈린다.